# ğŸ·ï¸ POS Tagging Fine-Tuning with Transformers

Welcome to the **POS Tagging Fine-tuning Repository**! This project fine-tunes transformer-based models for **Part-of-Speech (POS) tagging** using **CoNLL-U formatted datasets**. We provide two training approaches:  

âœ… **Standard Fine-tuning** â€“ Trains the model normally.  
ğŸ”¥ **Gradual Unfreezing Fine-tuning** â€“ Gradually unfreezes layers for better adaptation. 


## ğŸ“Œ Features  
- ğŸ— Supports **any transformer-based model** from Hugging Face ğŸ¤—.  
- ğŸ“– Reads data in **CoNLL-U format**.  
- ğŸš€ **Gradual unfreezing** for stable training.  
- ğŸ“Š Computes **accuracy & classification reports**.  

---


## ğŸ“‚ Repository Structure
```
â”œâ”€â”€ ğŸ“ gradual_finetune.py   â†’ Fine-tuning with gradual unfreezing  
â”œâ”€â”€ âš¡ normal_finetune.py    â†’ Standard fine-tuning method  
â”œâ”€â”€ ğŸ“¦ requirements.txt      â†’ All dependencies listed here  
â”œâ”€â”€ ğŸ“ dataset/              â†’ Store your dataset files here  
â””â”€â”€ ğŸ“œ README.md             â†’ All the documentation you need!  
```

---

## ğŸ“Š Dataset Information
The dataset must be in **CoNLL-U format** and should contain sentences with their corresponding POS tags.
- Each token in the dataset should have its **form (word)** and **UPOS (Universal POS Tag)**.
- Example format:

```
# sent_id = 1
# text = This is an example sentence.
1	This	_	DET	_	_	_	_	_	_
2	is	_	VERB	_	_	_	_	_	_
3	an	_	DET	_	_	_	_	_	_
4	example	_	NOUN	_	_	_	_	_	_
5	sentence	_	NOUN	_	_	_	_	_	_
6	.	_	PUNCT	_	_	_	_	_	_
```


## ğŸ“„ Download the Dataset
### ğŸ“¥ You can find the actual dataset here:
### ğŸ”— Dataset Link

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
  git clone https://github.com/your-username/pos-tagging-finetune.git
  cd pos-tagging-finetune
```

### 2ï¸âƒ£ Install Dependencies
```bash
  pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Fine-Tuning Scripts
#### ğŸƒâ€â™‚ï¸ **Standard Fine-Tuning**
```bash
python normal_finetune.py \
    --train_file ./dataset/train.conllu \
    --valid_file ./dataset/valid.conllu \
    --test_file ./dataset/test.conllu \
    --model_path ./path/to/pretrained/model \
    --output_dir ./path/to/output/directory \
    --epochs 3 \
    --batch_size 16 \
    --learning_rate 2e-5
```

#### ğŸ”¥ **Gradual Unfreezing Fine-Tuning**
```bash
python gradual_finetune.py \
    --train_file ./dataset/train.conllu \
    --valid_file ./dataset/valid.conllu \
    --test_file ./dataset/test.conllu \
    --model_path ./path/to/pretrained/model \
    --output_dir ./path/to/output/directory \
    --epochs 3 \
    --batch_size 16 \
    --learning_rate 2e-5
```

---

## ğŸ“Œ Key Features
âœ… Supports **POS tagging** using transformer-based models (e.g., BERT, XLM-R, MURIL)  
âœ… Uses **CoNLL-U format datasets**  
âœ… Implements **label alignment** for subword tokenization  
âœ… **Gradual unfreezing** method to fine-tune large models efficiently  
âœ… **Custom learning rates** for different model components  
âœ… **Evaluation metrics** including accuracy & classification report  

---

## ğŸ“œ Requirements
The following packages are required:
```bash
transformers
numpy
scikit-learn
datasets
conllu
torch
argparse
```
Alternatively, install them all using:
```bash
pip install -r requirements.txt
```

Additionally, ensure you have:
- **Python 3.6+** ğŸ
- **CUDA 12.1** (for GPU acceleration) âš¡

---

## ğŸ“Š Evaluation & Metrics
After training, the script will evaluate on the test set and display:
- **Accuracy** ğŸ“Š
- **Macro & Weighted F1 Scores** ğŸ“ˆ
- **Detailed Classification Report** ğŸ“„

---



ğŸŒŸ *If you find this useful, don't forget to star â­ the repository!*

